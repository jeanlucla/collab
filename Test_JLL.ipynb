{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_JLL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeanlucla/collab/blob/master/Test_JLL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdVQsRo9Rakr",
        "colab_type": "code",
        "outputId": "9acf44a6-6b6f-4d8e-f761-55e90c537936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "!pip install GPUtil"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting GPUtil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JElR-5giTh6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import os to set the environment variable CUDA_VISIBLE_DEVICES\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import GPUtil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-Dtbm0mSgMJ",
        "colab_type": "code",
        "outputId": "6135f34b-ebbc-4635-e6fc-4183bac4c4d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Set CUDA_DEVICE_ORDER so the IDs assigned by CUDA match those from nvidia-smi\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "\n",
        "# Get the first available GPU\n",
        "DEVICE_ID_LIST = GPUtil.getFirstAvailable()\n",
        "DEVICE_ID = DEVICE_ID_LIST[0] # grab first element from list\n",
        "print ('Device List :' ,DEVICE_ID_LIST)\n",
        "\n",
        "\n",
        "# Set CUDA_VISIBLE_DEVICES to mask out all other GPUs than the first available device id\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(DEVICE_ID)\n",
        "\n",
        "# Since all other GPUs are masked out, the first available GPU will now be identified as GPU:0\n",
        "device = '/gpu:0'\n",
        "print('Device ID (unmasked): ' + str(DEVICE_ID))\n",
        "print('Device ID (masked): ' + str(0))\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device List : [0]\n",
            "Device ID (unmasked): 0\n",
            "Device ID (masked): 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8eq7hSFvfdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NVSIsDIUTjWM"
      },
      "source": [
        "# Nouvelle section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "55fc0338-7062-4c87-f033-fd34141394e1",
        "id": "irnfYKEKTjWI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "GPUs = GPUtil.getGPUs()\n",
        "GPUavailability = GPUtil.getAvailability(GPUs, maxLoad = \n",
        "                                         0.5, maxMemory = 0.5, includeNan=False, excludeID=[], excludeUUID=[])\n",
        "GPUtil.showUtilization(all=False, attrList=None, useOldCode=False)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  0% |  0% |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e346eed0-061c-4656-cc59-0a333b6f45b0",
        "id": "aJkz45y_TjV_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Run a minimum working example on the selected GPU\n",
        "# Start a session\n",
        "with tf.Session() as sess:\n",
        "    # Select the device\n",
        "    with tf.device(device):\n",
        "        # Declare two numbers and add them together in TensorFlow\n",
        "        a = tf.constant(12)\n",
        "        b = tf.constant(30)\n",
        "        result = sess.run(a+b)\n",
        "        print('a+b=' + str(result))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a+b=42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "94c71fc6-b868-49f4-e68c-dc64ae43a08b",
        "id": "s6wM4CT4TjVv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "import sys\n",
        "import ctypes\n",
        "\n",
        "\n",
        "# Some constants taken from cuda.h\n",
        "CUDA_SUCCESS = 0\n",
        "CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT = 16\n",
        "CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_MULTIPROCESSOR = 39\n",
        "CU_DEVICE_ATTRIBUTE_CLOCK_RATE = 13\n",
        "CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE = 36\n",
        "\n",
        "\n",
        "def ConvertSMVer2Cores(major, minor):\n",
        "    # Returns the number of CUDA cores per multiprocessor for a given\n",
        "    # Compute Capability version. There is no way to retrieve that via\n",
        "    # the API, so it needs to be hard-coded.\n",
        "    return {(1, 0): 8,\n",
        "            (1, 1): 8,\n",
        "            (1, 2): 8,\n",
        "            (1, 3): 8,\n",
        "            (2, 0): 32,\n",
        "            (2, 1): 48,\n",
        "            }.get((major, minor), 192)  # 3.0 and above\n",
        "\n",
        "\n",
        "libnames = ('libcuda.so', 'libcuda.dylib', 'cuda.dll')\n",
        "\n",
        "for libname in libnames:\n",
        "    try:\n",
        "      cuda = ctypes.CDLL(libname)\n",
        "    except OSError:\n",
        "      continue\n",
        "    else:\n",
        "       break\n",
        "else:\n",
        "    raise OSError(\"could not load any of: \" + ' '.join(libnames))\n",
        "\n",
        "nGpus = ctypes.c_int()\n",
        "name = b' ' * 100\n",
        "cc_major = ctypes.c_int()\n",
        "cc_minor = ctypes.c_int()\n",
        "cores = ctypes.c_int()\n",
        "threads_per_core = ctypes.c_int()\n",
        "clockrate = ctypes.c_int()\n",
        "freeMem = ctypes.c_size_t()\n",
        "totalMem = ctypes.c_size_t()\n",
        "\n",
        "result = ctypes.c_int()\n",
        "device = ctypes.c_int()\n",
        "context = ctypes.c_void_p()\n",
        "error_str = ctypes.c_char_p()\n",
        "\n",
        "\n",
        "result = cuda.cuInit(0)\n",
        "if result != CUDA_SUCCESS:\n",
        "    cuda.cuGetErrorString(result, ctypes.byref(error_str))\n",
        "    print(\"cuInit failed with error code %d: %s\" % (result, error_str.value.decode()))\n",
        "    \n",
        "result = cuda.cuDeviceGetCount(ctypes.byref(nGpus))\n",
        "if result != CUDA_SUCCESS:\n",
        "    cuda.cuGetErrorString(result, ctypes.byref(error_str))\n",
        "    print(\"cuDeviceGetCount failed with error code %d: %s\" % (result, error_str.value.decode()))\n",
        "\n",
        "print(\"Found %d device(s).\" % nGpus.value)\n",
        "\n",
        "\n",
        "for i in range(nGpus.value):\n",
        "    result = cuda.cuDeviceGet(ctypes.byref(device), i)\n",
        "    if result != CUDA_SUCCESS:\n",
        "       cuda.cuGetErrorString(result, ctypes.byref(error_str))\n",
        "       print(\"cuDeviceGet failed with error code %d: %s\" % (result, error_str.value.decode()))\n",
        "\n",
        "if cuda.cuDeviceGetName(ctypes.c_char_p(name), len(name), device) == CUDA_SUCCESS:\n",
        "       print(\"  Name: %s\" % (name.split(b'\\0', 1)[0].decode()))\n",
        "if cuda.cuDeviceComputeCapability(ctypes.byref(cc_major), ctypes.byref(cc_minor), device) == CUDA_SUCCESS:\n",
        "       print(\"  Compute Capability: %d.%d\" % (cc_major.value, cc_minor.value))\n",
        "if cuda.cuDeviceGetAttribute(ctypes.byref(cores), CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT, device) == CUDA_SUCCESS:\n",
        "       print(\"  Multiprocessors: %d\" % cores.value)\n",
        "       print(\"  CUDA Cores: %d\" % (cores.value * ConvertSMVer2Cores(cc_major.value, cc_minor.value)))  \n",
        "if cuda.cuDeviceGetAttribute(ctypes.byref(threads_per_core), CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_MULTIPROCESSOR, device) == CUDA_SUCCESS:\n",
        "       print(\"  Concurrent threads: %d\" % (cores.value * threads_per_core.value))\n",
        "if cuda.cuDeviceGetAttribute(ctypes.byref(clockrate), CU_DEVICE_ATTRIBUTE_CLOCK_RATE, device) == CUDA_SUCCESS:\n",
        "       print(\"  GPU clock: %g MHz\" % (clockrate.value / 1000.))\n",
        "if cuda.cuDeviceGetAttribute(ctypes.byref(clockrate), CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE, device) == CUDA_SUCCESS:\n",
        "       print(\"  Memory clock: %g MHz\" % (clockrate.value / 1000.))\n",
        "       "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1 device(s).\n",
            "  Name: Tesla T4\n",
            "  Compute Capability: 7.5\n",
            "  Multiprocessors: 40\n",
            "  CUDA Cores: 7680\n",
            "  Concurrent threads: 40960\n",
            "  GPU clock: 1590 MHz\n",
            "  Memory clock: 5001 MHz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0EeBaEq5TjVr",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}